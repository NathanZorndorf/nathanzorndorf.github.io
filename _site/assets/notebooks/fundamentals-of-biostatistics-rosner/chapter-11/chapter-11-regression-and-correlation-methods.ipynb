{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Correlation Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.2 General Concepts\n",
    "\n",
    "Population regression line is given as $y = \\alpha + \\beta x + e$\n",
    "- $\\beta$ represents the change in $y$ for a 1 unit increase in $x$, where $y$ represents the average value for the \"dependent\" variable for the population. \n",
    "- e ~ N(0, $\\sigma^2$)\n",
    "\n",
    "Sample regression line is given as $\\hat{y} = a + bx$\n",
    "- $\\hat{y}$ is the **predicted (or average)** value of y for a given value of x \n",
    "- $a$ and $b$ are estimates of $\\alpha$ and $\\beta$, respectively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.3 Fitting Regression Lines - Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strengths & Weaknesses:\n",
    "\n",
    "**Assumptions:**\n",
    "- Appropriate whenever the average residual for each given value of x is 0.\n",
    "    - Normality is not strictly required \n",
    "    - However, the normality assumption is necessary to perform hypothesis tests concerning regression parameters \n",
    "    \n",
    "Drawbacks:\n",
    "\n",
    "What if assumptions are violated:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.4 Inferences About Parameters from Regression Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criteria for regression lines that fit the data well vs those that do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F Test for Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for a non-zero relationship between x and y (that some of the variance in y is captured by x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hypothesis**:\n",
    "- H0: $\\beta = 0$\n",
    "- Ha: $\\beta \\neq 0$<br>\n",
    "\n",
    "**Test Statistic**:<br>\n",
    "- $F$ = Reg MS /Res MS, follows an $F$ distribution with 1 and n - 2 $df$ respectively. \n",
    "\n",
    "**Mechanics**:\n",
    "1. Python\n",
    "2. R<br>\n",
    "    `model <- lm(y ~ x)`<br>\n",
    "    `anova(model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$R^2$ is the proportion of the variance of y that is explained by x. \n",
    "   - if $R^2$ = 1, then if x is known, y can be predicted exactly without error. \n",
    "   - if $R^2$ = 0, then x gives no information as to what y is, and it's as if x provides no relevant information regarding y. \n",
    "   - For small $n$, a better measure of % variation of y explained by x is given by the adjusted $R^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t Test for Simple Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives the same p-value as an $F$ test, but also provides interval estimates for $\\beta$.\n",
    "\n",
    "**Hypothesis**:\n",
    "- H0: $\\beta = 0$\n",
    "- Ha: $\\beta \\neq 0$<br>\n",
    "\n",
    "**Test**:\n",
    "- two-sided t-test, assuming you're Ha is that $\\beta \\neq 0$<br>\n",
    "\n",
    "**Mechanics**:\n",
    "1. Python\n",
    "2. R<br>\n",
    "    `model <- lm(y ~ x)`<br>\n",
    "    `summary(model)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.5 Interval Estimation for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval Estimates for Regression Parameters\n",
    "Standard errors and confidence intervals for slope and intercept of regression lines. \n",
    "\n",
    "*See section 11.4 t-tests for generating standard errors and constructing interval estimates for regression parameters via computer*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval Estimates for Predictions Made from Regression Lines\n",
    "\n",
    "How accurate is the esimate $\\hat{y} = a + bx$?<br>\n",
    "*it depends on whether we are making predictions for one specific observation with value x (one specific boy with a given height), or for the mean value of all observations with value of x (all boys of a given height).*\n",
    "\n",
    "So, we can calculate a standard error and a confidence interval for:\n",
    "- the ***observed*** values of y for a given x\n",
    "    - The distribution of observed $y$ values for the subset of individuals with independent variable $x$ is normal with mean $=\\hat{y}=a+b x$ and standard error given by\n",
    "$se_{1}(\\hat{y})=\\sqrt{s_{y \\cdot x}^{2}\\left[1+\\frac{1}{n}+\\frac{(x-\\bar{x})^{2}}{L_{x x}}\\right]}$\n",
    "    - $100 x (1 - \\alpha)$% of the observed values will fall within the ***prediction interval*** given by: $\\hat{y} \\pm t_{n-2,1-\\alpha / 2} s e_{1}(\\hat{y})$\n",
    "- the ***average*** value of y for a given x\n",
    "    - The best estimate of the average value of $y$ for a given $x$ is $\\hat{y}=a+b x .$ Its standard error, denoted by $se_{2}(\\hat{y}),$ is given by $se_{2}(\\hat{y})=\\sqrt{s_{y \\cdot x}^{2}\\left[\\frac{1}{n}+\\frac{(x-\\bar{x})^{2}}{L_{x x}}\\right]}$\n",
    "    - a two sided $100 x (1 - \\alpha)$% confidence interval for the ***average value of $y$*** is given by: $\\hat{y} \\pm t_{n-2,1-\\alpha / 2} s e_{2}(\\hat{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Mechanics**:\n",
    "- R<br>\n",
    "    `model <- lm(y~x)`<br>\n",
    "    `newdata <- dataframe(x = c(x1, x2, ..., xk))`<br>\n",
    "    `predict(model, newdata, interval = \"prediction\")`: for the prediction interval for a single observation<br>\n",
    "    `predict(model, newdata, interval = \"confidence\", se.fit = TRUE)`: for the confidence interval for the average prediction $\\hat{y}$ for a given value of $x$<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.6 Assessing the Goodness of Fit of Regression Lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What assumptions are made in fitting a linear regression, and what are some possible situations that would make these assumptions not viable, and what can be done when you have a situation where the assumptions are not met?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions of linear regression \n",
    "\n",
    "*An alternative way to describe all four assumptions is that the errors, , are independent normal random variables with mean zero and constant variance, $\\sigma^2$.*\n",
    "\n",
    "1. **Linearity**:\n",
    "    - for any given value of x, the corresponding value of y has an average value a + Bx, which is a linear function of x.<br><br>\n",
    "2. **Equal-Variance** & **Normality** of residuals:\n",
    "    - for any given value of x, the corresponding value of y is normally distributed about a + Bx with the same variance for any x. normality assumption is important in small samples. <br><br>\n",
    "3. **Independence of error terms**:\n",
    "    - There is no special pattern in the residuals that would indicate hidden variables that are affecting the observations.\n",
    "    - for any two data points, the error terms $e_1$, $e_2$ are independent of each other. It's important that the samples be independent for accurate intepretation of p-values.<br><br>\n",
    "    \n",
    "4. **No Bad outliers** (from \"Statistics with R\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing the Validity of the Linear Regression Assumptions\n",
    "\n",
    "1. **Assessing the linearity assumption:**: can be checked with a scatter plot<br><br>\n",
    "2. **Assessing the equal variance and normality assumption:** Since the variance of the residuals about an observation $x$ changes depending on how far away $x$ is from $\\bar{x}$, **Studentized residuals** are used to check for normality and constant variance of the residuals, and are normalized such that their variance can be one-to-one compared across the entire spectrum of X ( or Y) values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjustments to satisfy the assumptions of linear regression\n",
    "\n",
    "2. **adjusting input to satisfy equal variance & normality of residuals assumption**:\n",
    "    - One commonly used strategy that can be employed if unequal residual variances are present is to transform the dependent variable (y) to a different scale. This type of transformation is called a **variance stabilizing transformation**. \n",
    "        - The **square-root transformation** is useful when the residual variance is proportional to the average value of y (e.g., if the average value goes up by a factor of 2, then the residual variance goes up by a factor of 2 also). \n",
    "        - The **ln transformation** is useful when the residual variance is proportional to the square of the average values (e.g., if the average value goes up by a factor of 2, then the residual variance goes up by a factor of 4).\n",
    "    - *If you do employ a transformation, make sure that the linearity assumption still holds.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standard Deviation of Residuals About the Fitted Regression line**\n",
    "\n",
    "Let $\\left(x_{i}, y_{i}\\right)$ be a sample point used in estimating the regression line, $y=\\alpha+\\beta x$.\n",
    "If $y=a+b x$ is the estimated regression line, and\n",
    "$\\hat{e}_{i}=$ residual for the point $\\left(x_{i}, y_{i}\\right)$ about the estimated regression line, then\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\hat{e}_{i}=y_{i}-\\left(a+b x_{i}\\right) \\text { and } \\\\\n",
    "\\qquad s d\\left(\\hat{e}_{i}\\right)=\\sqrt{\\hat{\\sigma}^{2}\\left[1-\\frac{1}{n}-\\frac{\\left(x_{i}-\\bar{x}\\right)^{2}}{L_{x x}}\\right]}\n",
    "\\end{array}\n",
    "$$\n",
    "The **Studentized residual** corresponding to the point $\\left(x_{i}, y_{i}\\right)$ is $\\hat{e}_{i} / s d\\left(\\hat{e}_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.7 The Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(from \"Statistics with R, Navarro): \"*Pearson correlations are basically the same thing as linear regressions with only a single predictor added to the model.*\"\n",
    "\n",
    "**covariance:** un-normalized metric representing the relationship between two variables X and Y. \n",
    "\n",
    "**correlation coefficient** ($\\rho$) Scaled/Normalized metric representing the relationship between X and Y. Ranges between -1 and 1. \n",
    "\n",
    "**sample (Pearson) correlation coefficient** ($r$) Sample estimate of the population correlation coefficient $\\rho$ \n",
    "= \n",
    "$$\\frac{\\left(\\text{sample covariance between x and y}\\right)}{\\text{(sample std. dev. of x)(sample std. dev. of y)}}$$\n",
    "\n",
    "\n",
    "\n",
    "**Assumptions:**\n",
    "- $x$ and $y$ are normally distributed. \n",
    "\n",
    "**Hypothesis Test**:\n",
    "- H0: correlation between two variables = 0\n",
    "- Ha: correlation between two variables != 0\n",
    "\n",
    "**Mechanics**:\n",
    "- R<br>\n",
    "    `cor.test( x = parenthood$dan.sleep, y = parenthood$dan.grump )`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relationship between Sample Regression Coefficient (b) and Sample Correlation Coefficient (r)\n",
    "\n",
    "The relationship between the sample regression coefficient $(b)$ and the sample correlation coefficient $(r)$:\n",
    "\n",
    "$b=\\frac{r s_{y}}{s_{x}}$\n",
    "\n",
    "**When should the regression coefficient be used, and when should the correlation coefficient be used?**\n",
    "- $b$ is for prediction. \n",
    "- $r$ is for quantifying the linear relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.8 Statistical Inference for Correlation Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-Sample t Test for Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mechanics**:\n",
    "- R<br>\n",
    "    `cor.test(x, y)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One sample z Test for a Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The problem with using the t test formation in Equation 11.20 is that the sample correlation coefficient r has a skewed distribution for nonzero ρ that cannot be easily approximated by a normal distribution.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hypotheses**:<br>\n",
    "H0: ρ = ρ0<br>\n",
    "Ha: ρ ≠ ρ0<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interval Estimation for Correlation Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Size Estimation for Correlation Coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two-Sample Test for Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11.9 Multiple Regression\n",
    "\n",
    "population relationship: $$y=\\alpha+\\beta_{1} x_{1}+\\beta_{2} x_{2}+e$$\n",
    "\n",
    "The βj, j = 1, 2,..., k are referred to as partial-regression coefficients. βj represents the **average increase in y per unit increase in xj, with all other variables held constant** (or stated another way, after adjusting for all other variables in the model), and **is estimated by the parameter bj**.\n",
    "\n",
    "sample estimate: $$y=a+b_{1} x_{1}+b_{2} x_{2}+e$$\n",
    "\n",
    "The relationship between y and a specific independent variable xl is characterized as follows:\n",
    "\\begin{aligned}\n",
    "&y \\text { is normally distributed with expected value }=\\alpha_{\\ell}+\\beta_{\\ell} x_{\\ell} \\text { and variance } \\sigma^{2} \\text { where }\\\\\n",
    "&\\alpha_{\\ell}=\\alpha+\\beta_{1} x_{1}+\\cdots+\\beta_{\\ell-1} x_{\\ell-1}+\\beta_{\\ell+1} x_{\\ell+1}+\\cdots+\\beta_{k} x_{k}\n",
    "\\end{aligned}\n",
    "\n",
    "\n",
    "**Assumptions**:\n",
    "1. **All of the assumptions from single-variable linear regression** PLUS\n",
    "1. **Uncorrelated Predictors** - no collinearity. \n",
    "\n",
    "A **partial-residual plot** is a good way to check the validity of the assumptions. The **partial-residual plot** reflects the relationship between y and xl after each variable is adjusted for all other predictors in the multiple-regression model, which is a primary goal of performing a multiple-regression analysis.\n",
    "\n",
    "It can be shown that if the multiple-regression model in Equation 11.29 holds, then the residuals in step 1 should be linearly related to the residuals in step 2 with slope = βl (i.e., the partial-regression coefficient pertaining to xl in the multiple-regression model in Equation 11.29) and constant residual variance σ2. A separate partial-residual plot can be constructed relating y to each predictor x1,..., xk.\n",
    "\n",
    "*If there are strong relationships among the independent variables in a multiple-regression model, then the partial-regression coefficients may differ considerably from the simple linear-regression coefficients obtained from considering each independent variable separately.*\n",
    "\n",
    "**Mechanics**:\n",
    "1. R<br>\n",
    "    `lm(y ~ x1 + x2 + x3, data = data.frame)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing regression coefficients in multiple linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**If you want to compare multiple regression coefficients to each other directly, you must standardize the coefficients.**\n",
    "\n",
    "The **standardized regression coefficient** \n",
    "$$b_s = b × (s_x /s_y)$$\n",
    "\n",
    "It represents the estimated average increase in y (expressed in standard deviation units of y) per stan- dard deviation increase in x, after adjusting for all other variables in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the overall hypothesis that age and birthweight when considered together are significant predictors of blood pressure. How can this be done?\n",
    "\n",
    "$F$ Test:<br>\n",
    "H0: β1 = β2 = . . . = βk = 0<br>\n",
    "Ha: at least one of β1,..., βk ≠ 0<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significant p-value for this test could be attributed to either variable. We would like to perform significance tests to identify the independent contributions of each variable. How can this be done?\n",
    "\n",
    "In general, if we have k independent variables, then to assess the specific effect of the lth independent variable (xl), on y after controlling for the effects of all other variables:<br>\n",
    "H0: βl = 0, all other βj ≠ 0<br>\n",
    "H1: all βj ≠ 0<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**confounding variables**:<br>\n",
    "It is possible that an independent variable (x1) will seem to have an important effect on a dependent variable (y) when considered by itself but will not be signifi- cant after adjusting for another independent variable (x2). This usually occurs when x1 and x2 are strongly related to each other and when x2 is also related to y. We refer to x2 as a confounder of the relationship between y and x1. \n",
    "\n",
    "**collinear variables**:<br>\n",
    "In some instances, two strongly related variables are entered into the same multiple-regression model and, after controlling for the effect of the other variable, neither variable is significant. Such variables are referred to as collinear. It is best to avoid using highly collinear variables in the same multiple-regression model because their simultaneous presence can make it impossible to identify the specific effects of each variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goodness of Fit / Model Evaluation\n",
    "\n",
    "**Externally Studentized residual**: *i*th data point not used in the estimation of regression parameters. Used in the case of outliers to remove their effect from the regression. \n",
    "\n",
    "**Internally Studentized residual**: because the ith data point was used in estimating the regression parameters.\n",
    "\n",
    "**R<sup>2</sup>** (i.e.  the Coefficient of Determination): it is the proportion of the variance in the outcome variable that can be accounted for by the predictor. \n",
    "- 1- R<sup>2</sup> quantifies the amount of variance left over in the independent variable after the model is taken into account\n",
    "    - R<sup>2</sup> quantifies the reduction in variance after the model is taken into account.  \n",
    "\n",
    "**Adjusted R<sup>2</sup>**: Adding more predictors to the model will always increase the R<sup>2</sup>, so the adjusted R<sup>2</sup> only increases when a new predictor actually increases the performance of the model. \n",
    "- $\\text{Adjusted } R^{2}=1-\\left(\\frac{n-1}{n-p}\\right)\\left(1-R^{2}\\right)$\n",
    "- This is useful to use when comparing different multiple linear regression models, as the number of terms in the model won't inflate the adjusted R<sup>2</sup> like it will for the un-adjusted R<sup>2</sup>. However, the adjusted R<sup>2</sup> does not have the same interpretation as the un-adjusted R<sup>2</sup> (proportion of the variance explained in the dependent variable by the predictors).\n",
    "- Also useful for determining which coefficients are additive to the model, since the adjusted R<sup>2</sup> won't increase unless a predictor actually increases the explaine variance in the independent variable. \n",
    "\n",
    "**Partial R<sup>2</sup>** (i.e.  the Coefficient of Partial Determination): The proportion of the variance in model B that is not explained by model A, i.e. how much more variance can be accounted for by including the additional predictors in model B (with respect to model A).\n",
    "\\begin{aligned}\n",
    "R_{y, B \\mid A}^{2} &=\\frac{\\operatorname{SSR}(B \\mid A)}{\\operatorname{SSE}(A)} \\\\\n",
    "&=\\frac{\\operatorname{SSE}(A)-\\operatorname{SSE}(A, B)}{\\operatorname{SSE}(A)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
